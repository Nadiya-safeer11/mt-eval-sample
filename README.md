# Machine Translation Evaluation Sample

This is a simple project to evaluate the quality of a machine-translated text using the BLEU score.

---

## ğŸ“‚ Project Structure
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nadiya-safeer11/mt-eval-sample/blob/main/run.ipynb)


```
mt-eval-sample/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reference.txt
â”‚   â””â”€â”€ hypothesis.txt
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ run.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“‹ Description

This script uses [SacreBLEU](https://github.com/mjpost/sacrebleu) to evaluate a machine translation output (`hypothesis.txt`) against a reference translation (`reference.txt`).

---

## âš™ï¸ Requirements

Install the required package:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run

```bash
python run.py
```

### âœ… Expected Output

```
BLEU Score: 72.50
```

---

## ğŸ“˜ Example

- `reference.txt`:
  ```
  This is a test sentence.
  The quick brown fox jumps over the lazy dog.
  ```

- `hypothesis.txt`:
  ```
  This is a test sentence.
  The quick brown fox jumped over a lazy dog.
  ```

---

## ğŸ”— License

This project is free to use and modify for educational purposes.
